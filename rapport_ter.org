#+TITLE: Rapport de TER appariement de questions/réponses
#+AUTHOR: Félix Jamet
#+OPTIONS: tags:nil
#+LATEX_HEADER: \newcommand{\norm}[1]{\left\lVert#1\right\rVert}
#+LATEX_HEADER: \newcommand{\var}[1]{{\operatorname{\mathit{#1}}}}
#+LATEX_HEADER: \let\oldtextbf\textbf
#+LATEX_HEADER: \renewcommand{\textbf}[1]{\textcolor{red}{\oldtextbf{#1}}}
#+PROPERTY: header-args:ipython :eval no-export :results output drawer replace :exports results
* noweb                                                            :noexport:
:PROPERTIES:
:header-args:ipython: tangle: no :eval never
:END:

#+NAME: traincorpus
#+BEGIN_SRC ipython
  corpora = {'train2016p1': 'SemEval2016-Task3-CQA-QL-train-part1.xml'}
#+END_SRC

#+NAME: devcorpus
#+BEGIN_SRC ipython
  corpora = {'dev': 'SemEval2017-Task3-CQA-QL-dev.xml'}
#+END_SRC

#+NAME: bothyears
#+BEGIN_SRC ipython
  corpora = {'2016': 'SemEval2016-Task3-CQA-QL-test-input.xml',
             '2017': 'SemEval2017-task3-English-test-input.xml',}

  relevancy = {'2016': 'scorer/SemEval2016-Task3-CQA-QL-test.xml.subtaskB.relevancy',
               '2017': 'scorer/SemEval2017-Task3-CQA-QL-test.xml.subtaskB.relevancy'}
#+END_SRC

#+NAME: scoreutils
#+BEGIN_SRC ipython
  import subprocess
  from plasem_algostruct import transformtree

  def compute_relqs_scores(orgqnode, scorer):
      return {relid: scorer(orgqnode['org'], orgqnode[relid])
              for relid in orgqnode.keys() - {'org'}}

  def make_score_tree(document_tree, scorer):
      return transformtree(
          lambda x: compute_relqs_scores(x, scorer),
          document_tree,
          0
      )

  def getmapscore(predfilename):
      score = subprocess.run(
          ['./extractMAP.sh', predfilename], stdout=subprocess.PIPE)
      return score.stdout.decode('utf-8').strip('\n')

  from collections import Iterable
  def flatten(*args):
      for el in args:
          if isinstance(el, Iterable) and not isinstance(el, (str, bytes)):
              yield from flatten(*el)
          else:
              yield el

  def getpredfilename(*args):
      return 'predictions/rapport_' + '_'.join(flatten(args, 'scores.pred'))

  def orgmodetable(matrix, header=False):
      maxlen = [0] * len(matrix[0])
      for line in matrix:
          for i, cell in enumerate(line):
              if len(maxlen) <= i or len(str(cell)) > maxlen[i]:
                  maxlen[i] = len(str(cell))

      def orgmodeline(line, fill=' '):
          joinsep = fill + '|' + fill
          return '|' + fill + joinsep.join(
              str(cell) + fill * (mlen - len(str(cell)))
              for cell, mlen in zip(line, maxlen)
          ) + fill + '|'

      result = ''
      if header:
          result = orgmodeline(matrix[0]) + '\n' + \
              orgmodeline(('-') * len(maxlen), fill='-') + '\n'
          matrix = matrix[1:]
      result += '\n'.join(orgmodeline(line) for line in matrix)
      return result

#+END_SRC

#+NAME: loaddoctrees
#+BEGIN_SRC ipython
  import spacy
  from plasem_taln import inverse_document_frequencies
  from plasem_semeval import make_or_load_semeval_document_tree
  from semeval_xml import get_semeval_content

  nlp = spacy.load('en')
  doctrees = {
      corpus: make_or_load_semeval_document_tree(
          corpusxml,
          'spacy_en_' + corpus + '_questions.pickle',
          nlp,
          get_semeval_content)
      for corpus, corpusxml in corpora.items()
  }

  training_file = 'SemEval2016-Task3-CQA-QL-train-part1.xml'
  traindoctree = make_or_load_semeval_document_tree(
      training_file,
      'spacy_en_train2016p1_questions.pickle',
      nlp,
      get_semeval_content)

  inversedocfreqs = inverse_document_frequencies(
      [[str(tok) for tok in doc]
       for org in traindoctree.values()
       for doc in org.values()]
  )
  outofcorpusvalue = max(inversedocfreqs.values())

  context = {'inversedocfreqs': inversedocfreqs,
             'outofcorpusvalue': outofcorpusvalue}
#+END_SRC

#+NAME: scoringboilerplate
#+BEGIN_SRC ipython :noweb yes
  <<scoreutils>>

  <<loaddoctrees>>
 
  <<filters>>

  restable = []
#+END_SRC

#+NAME: makescores
#+BEGIN_SRC ipython
  from plasem_semeval import write_scores_to_file
  from plasem_taln import comparator

  comp = comparator(context, similarity)
  scores = make_score_tree(
      doctrees[corpus],
      comp.getscore
  )
#+END_SRC

#+NAME: writescores
#+BEGIN_SRC ipython
  predfile = getpredfilename(methodname, corpus, *rest)
  write_scores_to_file(scores, predfile)
#+END_SRC

#+NAME: debugmap
#+BEGIN_SRC ipython
  descr = ', '.join(flatten(corpus, rest))
  print('MAP%s:' % descr, MAP_from_semeval_relevancy(relevancy[corpus], scores))
#+END_SRC

#+NAME: MAP_from_relevancy
#+BEGIN_SRC ipython
  from plasem_semeval import MAP_from_semeval_relevancy
  restable.append([*(description_functions[i](value)
                     for i, value in enumerate((corpus, *rest))),
                   '%.2f' % (100 * MAP_from_semeval_relevancy(relevancy[corpus], scores))])
#+END_SRC

#+NAME: MAP_from_xml
#+BEGIN_SRC ipython
  from plasem_semeval import MAP_from_semeval_xml
  restable.append([*(description_functions[i](value)
                     for i, value in enumerate((corpus, *rest))),
                   '%.2f' % (100 * MAP_from_semeval_xml(corpora[corpus], scores))])
#+END_SRC

#+NAME: restable_viaSH
#+BEGIN_SRC ipython
  restable = [[*(description_functions[i](parameter_values[i])
                 for i in range(0,len(parameter_values))),
               getmapscore(getpredfilename(methodname, *parameter_values))]
              for parameter_values in parameters]
#+END_SRC

#+NAME: print_sorted_restable
#+BEGIN_SRC ipython
  restable.sort(key=lambda x: x[-1], reverse=True)
  restable.sort(key=lambda x: x[0])
  restable.insert(0, parameters_description)

  print('#+NAME:', methodname)
  print('#+CAPTION:', caption)
  print(orgmodetable(restable, header=True))
  print()
#+END_SRC

** Filtres
#+NAME: filters
#+BEGIN_SRC ipython
  MAPPSENT_STOPWORDS = set(open('stopwords_en.txt', 'r').read().splitlines())

  def isnotstopword(word):
      return word not in MAPPSENT_STOPWORDS

  lenfilters = {
      'gtr1': lambda word: len(word) > 1,
      'gtr2': lambda word: len(word) > 2,
      'gtr3': lambda word: len(word) > 3,
      'gtr4': lambda word: len(word) > 4,
  }

  nolenfilters = {
      'nostopwords': isnotstopword,
  }

  filters = {}
  filters.update(lenfilters)
  filters.update(nolenfilters)
  filters.update({ 'nofilter': lambda x: True })

  all_filters_descr = {
      'gtr1': '$\leq 1$',
      'gtr2': '$\leq 2$',
      'gtr3': '$\leq 3$',
      'gtr4': '$\leq 4$',
      'nostopwords': 'Mots outils',
      'nofilter': 'Pas de filtre',
  }

  all_indicators_descr = {
      'named_entities_text': 'Textes des entités nommées',
      'named_entities_label': 'Étiquettes des entités nommées',
      'tokens': 'Tokens',
      'lemmas': 'Lemmes',
  }

  def get_filters_descr(filters):
      return ', '.join(all_filters_descr[key] for key in filters)

  def get_indicator_descr(indicator):
      return all_indicators_descr[indicator]

  def get_doctree_descr(doctree):
      return all_doctrees_descr[doctree]

  from itertools import combinations
  def nonemptypartitions(iterable):
      for i in range(1, len(iterable) + 1):
          for perm in combinations(iterable, i):
              yield perm


  def join_predicates(iterable_preds):
      def joinedlocal(element):
          for pred in iterable_preds:
              if not pred(element):
                  return False
          return True
      print('joining', *(pred for pred in iterable_preds))
      return joinedlocal


  filters_partition = list(nonemptypartitions(nolenfilters))

  for len_and_nolen in product(nolenfilters, lenfilters):
      filters_partition.append(len_and_nolen)

  for lenfilter in lenfilters:
      filters_partition.append((lenfilter,))

  filters_partition.append(('nofilter',))
#+END_SRC

* Scripts                                                          :noexport:
#+BEGIN_SRC  ipython :eval no-export :noweb yes :session ponderation :tangle ponderation.py :shebang "#!/usr/bin/env python3" :results output drawer replace
  from itertools import product, count
  # <<traincorpus>>
  # <<devcorpus>>
  <<bothyears>>
  <<scoringboilerplate>>

  from plasem_taln import generic_weights_scorer, noun_weighter

  methodname = 'noun_ponderation'
  caption = 'Semeval - Scores MAP - Méthodes de référence'

  def frange(start, end=None, inc=1.0):
      if end is None:
          end = start + 0.0 # Ensure a float value for 'end'
          start = 0.0
      for i in count():
          next = start + i * inc
          if (inc>0.0 and next>=end) or (inc<0.0 and next<=end):
              break
          yield next

  ponderations = list(frange(0, 1, 0.02))

  parameters = list(product(corpora, ponderations))
  parameters_description = ('Édition', 'Ponderation', 'Score MAP')
  description_functions = [lambda x: x, lambda x: '%.2f' % (x)]

  for corpus, *rest in parameters:
      def noun_weight_similarity(context, reference, candidate):
          return generic_weights_scorer(context,
                                        reference,
                                        candidate,
                                        [(rest[0], noun_weighter)])
      similarity = noun_weight_similarity
      context['filters'] = [filters['gtr2']]
      context['wordex'] = lambda x: str(x).lower()

      <<makescores>>
      # <<MAP_from_xml>>
      <<MAP_from_relevancy>>

  <<print_sorted_restable>>
#+END_SRC

#+RESULTS:
:RESULTS:
#+NAME: noun_ponderation
#+CAPTION: Semeval - Scores MAP - Méthodes de référence
| Édition | Ponderation | Score MAP |
|---------|-------------|-----------|
| 2016    | 0.96        | 74.69     |
| 2016    | 0.98        | 74.67     |
| 2016    | 0.70        | 74.62     |
| 2016    | 0.72        | 74.52     |
| 2016    | 0.66        | 74.51     |
| 2016    | 0.74        | 74.40     |
| 2016    | 0.94        | 74.40     |
| 2016    | 0.76        | 74.37     |
| 2016    | 0.68        | 74.34     |
| 2016    | 0.78        | 74.33     |
| 2016    | 0.80        | 74.23     |
| 2016    | 0.46        | 74.16     |
| 2016    | 0.48        | 74.10     |
| 2016    | 0.92        | 74.05     |
| 2016    | 0.90        | 74.04     |
| 2016    | 0.82        | 74.02     |
| 2016    | 0.84        | 74.00     |
| 2016    | 0.88        | 73.99     |
| 2016    | 0.86        | 73.96     |
| 2016    | 0.50        | 73.87     |
| 2016    | 0.44        | 73.85     |
| 2016    | 0.58        | 73.83     |
| 2016    | 0.60        | 73.82     |
| 2016    | 0.40        | 73.73     |
| 2016    | 0.56        | 73.72     |
| 2016    | 0.64        | 73.70     |
| 2016    | 0.54        | 73.66     |
| 2016    | 0.42        | 73.61     |
| 2016    | 0.52        | 73.57     |
| 2016    | 0.62        | 73.51     |
| 2016    | 0.38        | 73.41     |
| 2016    | 0.34        | 73.20     |
| 2016    | 0.36        | 73.19     |
| 2016    | 0.32        | 72.82     |
| 2016    | 0.30        | 72.43     |
| 2016    | 0.28        | 72.09     |
| 2016    | 0.24        | 71.86     |
| 2016    | 0.26        | 71.86     |
| 2016    | 0.22        | 70.98     |
| 2016    | 0.20        | 70.94     |
| 2016    | 0.18        | 70.87     |
| 2016    | 0.16        | 70.69     |
| 2016    | 0.14        | 70.26     |
| 2016    | 0.08        | 70.14     |
| 2016    | 0.12        | 70.12     |
| 2016    | 0.10        | 70.11     |
| 2016    | 0.06        | 69.94     |
| 2016    | 0.04        | 69.60     |
| 2016    | 0.02        | 68.68     |
| 2016    | 0.00        | 67.20     |
| 2017    | 0.40        | 46.99     |
| 2017    | 0.42        | 46.72     |
| 2017    | 0.38        | 46.61     |
| 2017    | 0.36        | 46.55     |
| 2017    | 0.34        | 46.52     |
| 2017    | 0.44        | 46.47     |
| 2017    | 0.24        | 46.22     |
| 2017    | 0.60        | 46.22     |
| 2017    | 0.62        | 46.18     |
| 2017    | 0.48        | 46.17     |
| 2017    | 0.22        | 46.16     |
| 2017    | 0.46        | 46.16     |
| 2017    | 0.64        | 46.15     |
| 2017    | 0.66        | 46.14     |
| 2017    | 0.50        | 46.07     |
| 2017    | 0.32        | 46.02     |
| 2017    | 0.30        | 46.00     |
| 2017    | 0.68        | 45.97     |
| 2017    | 0.58        | 45.91     |
| 2017    | 0.26        | 45.87     |
| 2017    | 0.28        | 45.81     |
| 2017    | 0.56        | 45.81     |
| 2017    | 0.70        | 45.79     |
| 2017    | 0.52        | 45.71     |
| 2017    | 0.54        | 45.71     |
| 2017    | 0.20        | 45.49     |
| 2017    | 0.94        | 45.48     |
| 2017    | 0.92        | 45.47     |
| 2017    | 0.18        | 45.45     |
| 2017    | 0.78        | 45.44     |
| 2017    | 0.96        | 45.43     |
| 2017    | 0.98        | 45.40     |
| 2017    | 0.76        | 45.34     |
| 2017    | 0.72        | 45.25     |
| 2017    | 0.16        | 45.22     |
| 2017    | 0.88        | 45.18     |
| 2017    | 0.90        | 45.14     |
| 2017    | 0.74        | 45.11     |
| 2017    | 0.80        | 44.94     |
| 2017    | 0.82        | 44.74     |
| 2017    | 0.84        | 44.74     |
| 2017    | 0.14        | 44.64     |
| 2017    | 0.86        | 44.62     |
| 2017    | 0.12        | 44.40     |
| 2017    | 0.10        | 43.81     |
| 2017    | 0.08        | 42.38     |
| 2017    | 0.06        | 42.12     |
| 2017    | 0.04        | 41.51     |
| 2017    | 0.00        | 40.72     |
| 2017    | 0.02        | 40.58     |

:END:


* Présentation du sujet
Avec la démocratisation d'Internet, les communautés de questions/réponses telles que StackExchange, AskUbuntu ou Quora ont proliféré, produisant ainsi une masse conséquente d'informations.

Il est paradoxalement difficile de trouver la réponse idéale à une question parmi une telle quantité d'informations. C'est ce problème qui est investigué depuis 2015 dans le cadre de la tâche 3 de SemEval[fn:semevaltask3].

[fn:semevaltask3] Voir http://alt.qcri.org/semeval2017/task3/.


SemEval est un /WorkShop/ ayant pour but d'évaluer des approches d'interprétation automatique de la sémantique du langage naturel.
La tâche 3 est concernée par l'appariement questions/réponses, autrement dit, étant donné une question, trouver la ou les meilleures réponses y correspondant.

La sous-tâche B de la tâche 3 de SemEval consiste à faire de l'appariement question/question, c'est à dire étant donné deux questions, déterminer elles sont similaires.
C'est cette sous-tâche qui est traitée dans ce TER.

Les méthodes proposées par les participants des éditions 2016 et 2017 de SemEval pour résoudre cette étape sont des techniques d'apprentissage automatique allant du SVM au /deep learning/. *(pas sûr ; le rapport de semeval 2016 stipule de (Franco-Salvador et al., 2016) que "They use distributed representations of words, knowledge graphs generated with BabelNet, and frames from FrameNet.", j'ai pas l'impression que c'est du /machine learning/)*

Ce TER a pour but de regarder l'efficacité que pourrait avoir une approche plus simple, d'avantage ancrée dans la linguistique computationnelle que dans l'apprentissage automatique.
Pour cela, trois objectifs sont distingués :
 - Réfléchir à la manière de détecter des questions similaires en termes de caractéristiques linguistiques.(d'indicateurs et de marqueurs linguistiques.)
 - Mettre en oeuvre une méthode pour les détecter automatiquement.
 - Travailler sur une évaluation de l'approche proposée.

* SemEval - Tâche 3 - Similarité question/question
Afin de comparer différentes approches, SemEval fournit des données d'entrainement, de développement et de test, ainsi qu'un /scorer/ permettant d'associer un score à une approche.

Les données proviennent des forums de /Qatar Living/, un site centré autour d'une communauté d'expatriés vivant au Qatar.
Ces données sont organisées sous forme de documents XML contenant $N$ questions dites originales.
À chaque question originale sont associées 10 questions reliées, et à chaque question reliée sont associés 10 commentaires.

Dans le cadre de la sous-tâche B, il est demandé à une approche d'ordonner les questions reliées selon leur pertinence par rapport à la question originale.
Le /scorer/ utilise cet ordre prédit pour évaluer une approche, en utilisant l'indicateur MAP (/Mean Average Precision/).
La MAP d'une liste de prédiction reflète non seulement l'exactitude des prédictions mais également à quel point l'ordre des prédictions est correct.

* Méthodes proposées
Plusieurs méthodes sont envisagées. Il a été choisi de mesurer leur efficacité individuellement dans un premier temps, puis de chercher des manières pour les combiner les unes avec les autres.

Le problème est abordé comme un problème de /scoring/. Il s'agit de donner un score de similarité entre une question reliée et sa question originale. L'ordre des questions est établi en triant les questions dans l'ordre décroissant selon leur score de similarité.

Les méthodes proposées utilisent des sacs de mots. Une approche à base de sacs de mots consiste à représenter un document en utilisant un multi-ensemble de mots (un multi-ensemble est un ensemble pouvant contenir plusieurs occurences d'un même élément).
Une conséquence de cette approche est que l'ordre des mots est ignoré.

Un autre point commun des méthodes envisagées est qu'elles ont toutes recours au TF-IDF pour pondérer les mots selon leur fréquence.
La TF (/Term Frequency/) d'un mot correspond à sa fréquence d'apparition dans l'ensemble des documents.
L'IDF (/Inverse Document Frequency/) d'un mot est proportionnelle à l'inverse du nombre de documents dans lesquels il apparaît.
La TF-IDF d'un mot présent dans le corpus correspond à une combinaison de sa TF et de son IDF.
Si un mot est absent du corpus, sa TF-IDF est égale à l'IDF du terme le plus rare. *pas sûr que ça soit la bonne manière de faire*
Plus formellement,

\[TF(terme, document) = \frac{occurences(terme, document)}{taille(document)}\]
\[IDF(terme, corpus) = log \left( \frac{taille(corpus)}
{\norm{\{doc / doc \in corpus \land terme \in doc\}}} \right)\]
\[\var{TF-IDF}(terme, document, corpus) =
\begin{cases}
TF(terme, document) \times IDF(terme, corpus) & \text{si } terme \in corpus \\
\max(\{IDF(el, corpus) / el \in corpus\}) & \text{sinon}
\end{cases}\]

Les méthodes sont testées sur les données de test des années 2016 et 2017 de la tâche 3 de SemEval et sont entrainées sur les données d'entrainement fournies pour ces années.

** Mise au point d'une méthode de référence
La méthode de référence que nous proposons consiste à travailler avec l'intersection des sacs de mots des questions à comparer. Le score de similarité est établi comme la somme des valeurs TF-IDF des sacs de mots. Les valeurs TF-IDF sont calculées en utilisant comme corpus le texte de toutes les questions originales et reliées extraites des données d'entrainement et comme document les deux questions mises bout à bout.

\[similarit\acute{e}_{r\acute{e}f\acute{e}rence}(Q_1, Q_2) =
\sum_{terme \in Q_1 \cap Q_2} \var{TF-IDF}(terme, Q_1 \cup Q_2, texte_{entrainement})\]

La table [[baseline]] indique les scores MAP de la méthode de référence sur les éditions 2016 et 2017 de SemEval. La table [[best_scores]] montre les meilleurs scores MAP des éditions 2016 et 2017 de SemEval, ainsi que les scores de la IR (/Information Retrieval/) /baseline/, correspondant à conserver l'ordre initialement proposé par le moteur de recherche lors de la constitution des jeux de données.

#+BEGIN_SRC ipython :eval no-export :noweb yes :session baselineexec :tangle rapport_baseline.py :shebang "#!/usr/bin/env python3" :results output drawer replace
  from itertools import product
  <<bothyears>>
  <<scoringboilerplate>>

  from plasem_taln import baseline_similarity
  similarity = baseline_similarity

  methodname = 'baseline'
  caption = 'Semeval - Scores MAP - Méthode de référence'

  parameters = list(product(corpora))
  parameters_description = ('Édition', 'Score MAP')
  description_functions = [lambda x: x]

  for corpus, *rest in parameters:
      <<makescores>>
      <<writescores>>
      <<MAP_from_relevancy>>

  <<print_sorted_restable>>
#+END_SRC

#+RESULTS:
:RESULTS:
#+NAME: baseline
#+CAPTION: Semeval - Scores MAP - Méthode de référence
| Édition | Score MAP |
|---------|-----------|
| 2016    | 71.48     |
| 2017    | 44.21     |

:END:

#+NAME: best_scores
#+ATTR_LATEX: :placement [p]
#+CAPTION: SemEval - Meilleurs scores MAP
| Édition | Méthode               | Score MAP  |
|---------+-----------------------+------------|
|    2016 | UH-PRHLT-contrastive2 |      77.33 |
|    2016 | UH-PRHLT-primary      |      76.70 |
|    2016 | UH-PRHLT-contrastive1 |      76.56 |
|    2016 | IR /baseline/         |      74.75 |
|    2017 | KeLP-contrastive1     |      49.00 |
|    2017 | SimBow-contrastive2   |      47.87 |
|    2017 | SimBow-primary        |      47.22 |
|    2017 | IR /baseline/         |      41.85 |


** Filtres


#+BEGIN_SRC ipython :eval no-export :noweb yes :session baselinefiltersexec :tangle rapport_baseline_filters.py :shebang "#!/usr/bin/env python3" :results output drawer replace
  from itertools import product
  <<bothyears>>
  <<scoringboilerplate>>

  from plasem_taln import filters_baseline_similarity
  similarity = filters_baseline_similarity

  methodname = 'baseline_filters'
  caption = 'Semeval - Scores MAP - Méthodes de référence'

  parameters = list(product(corpora, filters_partition))
  parameters_description = ('Édition', 'Filtres', 'Score MAP')
  description_functions = [lambda x: x, get_filters_descr]
  for corpus, *rest in parameters:
      context['filters'] = [filters[key] for key in rest[0]]
      <<makescores>>
      <<writescores>>
      <<MAP_from_relevancy>>

  <<print_sorted_restable>>
#+END_SRC

#+RESULTS:
:RESULTS:
#+NAME: baseline_filters
#+CAPTION: Semeval - Scores MAP - Méthodes de référence
| Édition | Filtres               | Score MAP |
|---------|-----------------------|-----------|
| 2016    | Mots outils, $\leq 1$ | 75.42     |
| 2016    | Mots outils, $\leq 2$ | 75.04     |
| 2016    | $\leq 1$              | 74.58     |
| 2016    | $\leq 3$              | 74.42     |
| 2016    | Mots outils, $\leq 4$ | 74.21     |
| 2016    | $\leq 4$              | 74.06     |
| 2016    | Mots outils, $\leq 3$ | 73.97     |
| 2016    | $\leq 2$              | 73.87     |
| 2016    | Mots outils           | 73.76     |
| 2016    | Pas de filtre         | 73.19     |
| 2017    | $\leq 1$              | 46.89     |
| 2017    | Mots outils, $\leq 1$ | 46.35     |
| 2017    | Mots outils, $\leq 2$ | 46.08     |
| 2017    | $\leq 2$              | 46.07     |
| 2017    | $\leq 3$              | 45.59     |
| 2017    | Pas de filtre         | 45.56     |
| 2017    | Mots outils           | 45.53     |
| 2017    | Mots outils, $\leq 3$ | 45.46     |
| 2017    | Mots outils, $\leq 4$ | 41.80     |
| 2017    | $\leq 4$              | 40.47     |

:END:


** Lemmatisation
* Indicateurs envisagés
** Nature grammaticale des mots
Identifier la nature grammaticale d'un mot peut donner un indice sur sa pertinence.
Une hypothèse vraisemblable est qu'un nom participera davantage au sens d'une phrase, tandis qu'un adjectif aura moins d'impact.
La nature d'un mot peut être détectée à l'aide d'étiqueteurs morpho-syntaxiques.

** Reconnaissance d'entité nommées
Une entité nommée est un mot ou groupe de mots designant une entité du monde.
La reconnaissance d'entités nommées est une technique permettant de detecter ces entités et de les classifier.

Les classes de ces entité comprennent des noms d'individus, des produits, des villes, ou encore des entreprises.

La reconnaissance d'entités nommées est intéressante pour la problématique de ce TER car beaucoup de questions portent sur des entités nommées.


*** Multi-ensembles d'entités nommées
Étant donné une phrase $s$, on peut créer plusieurs multi-ensembles à partir de ses entités nommées :
 - $CAT_s$, le multi-ensemble contenant les catégories des entités nommées de la phrase $s$.
 - $TOK_s$, le multi-ensemble contenant les tokens des entités nommées de la phrase $s$.

En prenant $s = \text{"Google and Apple headquarters are in California"}$, on a
$CAT_s = \{\text{ORG}, \text{ORG}, \text{GPE}\}$ et $TOK_s = \{\text{Google}, \text{Apple}, \text{California}\}$


* Pré-traitements
L'étape de pré-traitement consiste à utiliser des techniques simples

** Mots-outils

** Filtres

** Majuscules

* Techniques de TALN (à incorporer au fur et à mesure de l'écriture)

** Racinisation (/stemming/) et lemmatisation
Le but des ces deux techniques est de modifier un mot (typiquement d'ôter des suffixes) afin d'en obtenir une forme simplifiée.
Cette forme simplifiée permet d'associer des mots dérivant d'une même racine.

La différence entre ces deux techniques se situe dans la forme simplifiée qu'elle permettent d'obtenir.
Un racinisateur permet de générer une racine (ou radical), qui est une forme artificielle, tandis qu'un lemmatisateur génère un lemme, correspondant à un terme d'usage.
Par exemple, le mot "chercher" pourra être racinisé en "cherch", mais sera lemmatisé en "cherche".

Cette technique a son utilité dans la tâche de modifier des textes afin de mieux pouvoir les comparer, car elle permet d'une part de donner une forme commune à des termes proches, les rendant ainsi directement comparable, et d'autre part de réduire la diversité linguistique tout en préservant la semantique.

* Perspectives

** Dictionnaire de synonymes (wordnet)

** Suppression de bruit
Signatures, nombres, "utilisation créative des signes de ponctuation", détection de fautes, obligations sociales et remerciements, abbréviations

** Vocabulaire spécifique
Comparer avec d'autres corpus (corpora ?) pour voir quels termes sont spécifiques à quatar living et réfléchir à comment traiter les termes spécifiques

** Forme interrogative
La forme interrogative est à priori simple à détecter en anglais et pourrait être un bon critère pour extraire les passages les plus importants.
