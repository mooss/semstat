#+TITLE:Appariement de questions/réponses
#+AUTHOR:Félix Jamet
# -*- org-export-babel-evaluate: t -*-
#+PROPERTY: header-args:ipython :session semexec :eval no-export :results output silent :exports results
#+OPTIONS: toc:nil title:nil

\newpage
* Évaluation des approches (=semeval_executable.py=)
:PROPERTIES:
:header-args: :ipython: :tangle semeval_executable.py :exports code :session semexec :eval no-export
:END:
** Imports

#+BEGIN_SRC ipython :shebang "#!/usr/bin/python"
  from itertools import product, combinations
  import spacy
  from spacy.lang.en.stop_words import STOP_WORDS
  from plasem_algostruct import *
  from plasem_taln import *
  from semeval_xml import get_semeval_content
#+END_SRC

** Paramètres d'exécution

#+BEGIN_SRC ipython
  debug_mode = False;
  seek_optimal_ner_ponderation = False
#+END_SRC

** Scores
Les scores sont stockés dans un arbre construit à partir de l'arbre des documents.
=compute_relqs_scores= calcule les scores de similarité d'un noeud de l'arbre des documents, en attribuant à chaque question relié son score obtenu en comparaison avec sa question originale.

#+BEGIN_SRC ipython
  def compute_relqs_scores(orgqnode, scorer):
      return {relid: scorer(orgqnode['org'], orgqnode[relid])
              for relid in orgqnode.keys() - {'org'}}
#+END_SRC

=make_score_tree= transforme le premier niveau d'un arbre de documents en lui appliquant =compute_relqs_scores= associé à la fonction de scoring recue en paramètre.

#+BEGIN_SRC ipython
  def make_score_tree(document_tree, scorer):
      return transformtree(
          lambda x: compute_relqs_scores(x, scorer),
          document_tree,
          0
      )
#+END_SRC

*** Écriture des fichiers de prédiction
Semeval fournit un script permettant de noter les performances d'une approche.
Ce script prend en entrée un fichier de prédiction dont chaque ligne correspond à une question reliée et est formatée de la manière suivante :

#+BEGIN_EXAMPLE
orgq_id  relq_id  0  score  true
#+END_EXAMPLE

Les troisième et cinquième colonnes sont sans intérêt pour cette tâche.

Le fichier de prédiction est destiné à être ensuite comparé à un fichier de référence de Semeval, afin d'évaluer les performances du système.

La fonction =write_scores_to_file= permet de générer ce fichier de prédiction.
Les résultats sont triés sur le tas, pour correspondre à l'ordre du fichier de références.

#+BEGIN_SRC ipython
  def write_scores_to_file(scores, filename, verbose=False):
      """Write a semeval score tree to a prediction file.

      Parameters
      ----------
      scores : dict of dict of float
          The scores to write.

      filename : str
         The name of the output file.
      """
      linebuffer = [(orgid, relid, str(0), str(score), 'true')
                    for orgid, relqs in scores.items()
                    for relid, score in relqs.items()]

      linebuffer.sort(key=lambda x: natural_sort_key(x[1]))

      if verbose:
          print('writing scores to', prediction_file)

      with open(filename, 'w') as out:
          out.write('\n'.join(['\t'.join(el) for el in linebuffer]))

#+END_SRC

** Dimensions orthogonales d'une approche
Plusieurs dimensions orthogonales sont envisagées pour appareiller des questions. Ces dimensions sont combinées les unes avec les autres, en faisant un produit cartésien, formant ainsi une approche.

*** Modèle de langage
Un seul modèle de langage est utilisé.
#+BEGIN_SRC ipython
  models = {
      'spacy_en': spacy.load('en')
  }
#+END_SRC

*** Corpus
Les approches sont testées sur les données 2016 et 2017 de Semeval.
#+BEGIN_SRC ipython

  if debug_mode:
      corpuses = {
          'debug': 'debug.xml',
      }
  else:
      corpuses = {
          '2016': 'SemEval2016-Task3-CQA-QL-test-input.xml',
          '2017': 'SemEval2017-task3-English-test-input.xml',
      }
#+END_SRC

*** Extraction de contenu
Deux manières d'extraire du contenu sont envisagées. Elles se différencient au niveau de l'extraction du contenu des questions reliées. La première extrait uniquement le sujet et le corps d'une question, tandis que la seconde extrait également les commentaires des questions reliées.

#+BEGIN_SRC ipython
  extractors = {
      'questions': lambda x: get_semeval_content(x).lower(),
     # 'questions_with_comments': get_semeval_content_with_relcomments
  }
#+END_SRC

Ces fonctions sont fournies dans le fichier =semeval_xml.py=.

*** Filtrage des mots
Les mots d'un sac de mots peuvent être filtrés ou non selon un prédicat.

#+BEGIN_SRC ipython
  MAPPSENT_STOPWORDS = set(open('stopwords_en.txt', 'r').read().splitlines())

  def isnotstopword(word):
      return word not in STOP_WORDS


  def isnotstopword2(word):
      return word not in MAPPSENT_STOPWORDS


  lenfilters = {
      'gtr1': lambda word: len(word) > 1,
      'gtr2': lambda word: len(word) > 2,
      'gtr3': lambda word: len(word) > 3,
      'gtr4': lambda word: len(word) > 4,
  }

  nolenfilters = {
      'nostopwords': isnotstopword2,
  }

  filters = {}
  filters.update(lenfilters)
  filters.update(nolenfilters)
  filters.update({ 'nofilter': lambda x: True })
#+END_SRC

La fonction =nonemptypartitions= est utilisée pour combiner les filtres.
#+BEGIN_SRC ipython
  def nonemptypartitions(iterable):
      for i in range(1, len(iterable) + 1):
          for perm in combinations(iterable, i):
              yield perm


  def join_predicates(iterable_preds):
      def joinedlocal(element):
          for pred in iterable_preds:
              if not pred(element):
                  return False
          return True
      print('joining', *(pred for pred in iterable_preds))
      return joinedlocal


  filters_partition = list(nonemptypartitions(nolenfilters))

  for len_and_nolen in product(nolenfilters, lenfilters):
      filters_partition.append(len_and_nolen)

  for lenfilter in lenfilters:
      filters_partition.append((lenfilter,))

  filters_partition.append(('nofilter',))
#+END_SRC

*** Extraction de mots et de phrases
Les sacs de mots sont construits à l'aide de deux fonctions.
La première est une fonction d'extraction de caractéristique, qui étant donné un token, renvoie la caractéristique désirée de celui-ci. La deuxième est une fonction d'extraction de phrase, qui étant donné un document, renvoie un itérable contenant des mots.

Chaque méthode de construction de sacs de mots utilise ces deux fonctions.
#+BEGIN_SRC ipython
  def extracttext(tok):
      return tok.text

  def extractlemma(tok):
      return tok.lemma_

  def extractlabel(ent):
      return ent.label_ if hasattr(ent, 'label_') else None

  def getentities(doc):
      return doc.ents or list()

  wordextractors = {
      'text': extracttext,
      'lemma': extractlemma,
      'label': extractlabel,
  }

  sentenceextractors = {
      'entities': getentities,
      'document': lambda x: x,
  }

  morphologic_indicators = {
      'tokens': ('text', 'document'),
      'lemmas': ('lemma', 'document'),
  }

  ner_indicators = {
      'named_entities_text': ('text', 'entities'),
      'named_entities_label': ('label', 'entities'),
  }

  all_indicators = {}
  all_indicators.update(morphologic_indicators)
  all_indicators.update(ner_indicators)

  def getindicatorfunctions(key):
      return (wordextractors[all_indicators[key][0]], sentenceextractors[all_indicators[key][1]])
#+END_SRC

*** Création des arbres de documents

#+BEGIN_SRC ipython

  training_file = 'SemEval2016-Task3-CQA-QL-train-part1.xml'

  training_doctree = make_or_load_document_tree(
      training_file,
      'train_2016_part1.pickle',
      models['spacy_en'],
      get_semeval_content,
      verbose=True
  )

  inversedocfreqs = {
      wordex + '_' + sentex: inverse_document_frequencies(
          [[wordextractors[wordex](tok) for tok in sentenceextractors[sentex](doc)]
           for org in training_doctree.values()
           for doc in org.values()]
      )
      for wordex, sentex in all_indicators.values()
  }

  out_of_corpus_value = max(inversedocfreqs['text_document'].values())
  doctrees = {
      '_'.join((model, corpus, extractor)): make_or_load_document_tree(
          corpuses[corpus],
          '_'.join((model, corpus, extractor) )+ '.pickle',
          models[model],
          extractors[extractor],
          verbose=True
      )
      for model, corpus, extractor in product(models, corpuses, extractors)
  }
#+END_SRC

** Méthodes                                                          :export:
=getpredfilename= permet de s'assurer que les noms des fichiers de prédiction sont tous construits de la même manière.

#+BEGIN_SRC ipython
  def getpredfilename(doctree, indicator, filterspartition, methodcategory):
      return 'predictions/' + '_'.join((doctree, indicator, *filterspartition,
                       methodcategory, 'scores.pred'))
#+END_SRC


Un script shell est utilisé pour extraire le score MAP d'un fichier de prédiction :
#+BEGIN_SRC sh :shebang "#!/usr/bin/env bash" :exports code :eval never :tangle extractMAP.sh
  prediction=$1

  if echo $prediction | grep --quiet "2016"
  then
      reference=scorer/SemEval2016-Task3-CQA-QL-test.xml.subtaskB.relevancy
  else
      if echo $prediction | grep --quiet "2017"
      then
          reference=scorer/SemEval2017-Task3-CQA-QL-test.xml.subtaskB.relevancy
      else
          reference=scorer/SemEval2016-debug.relevancy
      fi
  fi

  python2 scorer/ev.py $reference $prediction | grep "^MAP" | sed 's/ \+/;/g' | cut -f 4 -d ';'
#+END_SRC

*** hidden utils                                                   :noexport:

#+BEGIN_SRC ipython :tangle no :exports none :results silent
  import subprocess

  # def orgmodetable(matrix, header=False):
  #     maxlen = [0] * len(matrix[0])
  #     for line in matrix:
  #         for i, cell in enumerate(line):
  #             if len(maxlen) <= i or len(cell) > maxlen[i]:
  #                 maxlen[i] = len(cell)

  #     def orgmodeline(line, fill=' '):
  #         joinsep = fill + '|' + fill
  #         return '|' + fill + joinsep.join(
  #             cell + fill * (mlen - len(cell))
  #             for cell, mlen in zip(line, maxlen)
  #         ) + fill + '|'

  #     result = ''
  #     if header:
  #         result = orgmodeline(matrix[0]) + '\n' + \
  #             orgmodeline(('-') * len(maxlen), fill='-') + '\n'
  #         matrix = matrix[1:]
  #     result += '\n'.join(orgmodeline(line) for line in matrix)
  #     return result


  all_filters_descr = {
      'gtr1': '$\leq 1$',
      'gtr2': '$\leq 2$',
      'gtr3': '$\leq 3$',
      'gtr4': '$\leq 4$',
      'nostopwords': 'Mots outils',
      'nofilter': 'Pas de filtre',
  }

  all_indicators_descr = {
      'named_entities_text': 'Textes des entités nommées',
      'named_entities_label': 'Étiquettes des entités nommées',
      'tokens': 'Tokens',
      'lemmas': 'Lemmes',
  }

  all_doctrees_descr = {
      '_'.join((model, corpus, extractor)): corpus
      for model, corpus, extractor in product(models, corpuses, extractors)
  }

  def get_filters_descr(filters):
      return ', '.join(all_filters_descr[key] for key in filters)

  def get_indicator_descr(indicator):
      return all_indicators_descr[indicator]

  def get_doctree_descr(doctree):
      return all_doctrees_descr[doctree]

  def get_map_score(predfilename):
      score = subprocess.run(
          ['./extractMAP.sh', predfilename], stdout=subprocess.PIPE)
      return score.stdout.decode('utf-8').strip('\n')

#+END_SRC

*** Méthodes bruteforce
Les méthodes bruteforce correspondent à tester toutes les combinaisons d'arbres de documents, de sacs de mots et de filtres.
Les méthodes bruteforce sont crées en faisant le produit cartésien des dimensions envisagées.

Les méthodes précédemment générées sont exécutées et les scores produits sont écrits dans les fichiers correspondants.

#+BEGIN_SRC ipython
  bruteforce_methods = (doctrees, all_indicators, filters_partition)


  for doctree, indicator, filterspartition in product(*bruteforce_methods):
      wordex, sentex = all_indicators[indicator]
      customscorer = scorer(
          wordextractors[wordex],
          sentenceextractors[sentex],
          [filters[filterkey] for filterkey in filterspartition],
          inversedocfreqs[wordex + '_' + sentex],
          out_of_corpus_value,
          tf_idf_scorer
      )

      scores = make_score_tree(
          doctrees[doctree],
          customscorer.get_score
      )

      prediction_file = getpredfilename(doctree, indicator, filterspartition, 'bruteforce')
      write_scores_to_file(scores, prediction_file, verbose=True)
#+END_SRC

#+BEGIN_SRC ipython :exports results :results drawer output replace :tangle no :session semexec
  for doctree in doctrees:
      restable = [[get_indicator_descr(indi),
                   get_filters_descr(fltr),
                   get_map_score(getpredfilename(doctree, indi, fltr, 'bruteforce'))]
                  for indi, fltr in product(*bruteforce_methods[1:])]

      restable.sort(key=lambda x: x[2], reverse=True)
      restable.insert(0, ['Sac de mots', 'Filtres', 'Score MAP'])
      print('\\newpage\n' + '*année ' + all_doctrees_descr[doctree] + '*' + '\n')
      print(orgmodetable(restable, header=True))
      print()
#+END_SRC

#+RESULTS:
:RESULTS:
\newpage
*année 2016*

| Sac de mots                    | Filtres               | Score MAP |
|--------------------------------|-----------------------|-----------|
| Lemmes                         | Mots outils, $\leq 2$ | 0.7679    |
| Textes des entités nommées     | Mots outils           | 0.7493    |
| Textes des entités nommées     | Mots outils, $\leq 1$ | 0.7493    |
| Textes des entités nommées     | Mots outils, $\leq 2$ | 0.7493    |
| Textes des entités nommées     | Mots outils, $\leq 3$ | 0.7493    |
| Textes des entités nommées     | Mots outils, $\leq 4$ | 0.7493    |
| Textes des entités nommées     | $\leq 1$              | 0.7493    |
| Textes des entités nommées     | $\leq 2$              | 0.7493    |
| Textes des entités nommées     | $\leq 3$              | 0.7493    |
| Textes des entités nommées     | $\leq 4$              | 0.7493    |
| Textes des entités nommées     | Pas de filtre         | 0.7493    |
| Lemmes                         | Mots outils, $\leq 1$ | 0.7490    |
| Tokens                         | Mots outils, $\leq 2$ | 0.7488    |
| Tokens                         | Mots outils, $\leq 1$ | 0.7446    |
| Lemmes                         | Mots outils           | 0.7424    |
| Tokens                         | Mots outils, $\leq 3$ | 0.7423    |
| Tokens                         | Mots outils, $\leq 4$ | 0.7422    |
| Étiquettes des entités nommées | Mots outils, $\leq 4$ | 0.7369    |
| Étiquettes des entités nommées | $\leq 4$              | 0.7369    |
| Tokens                         | Mots outils           | 0.7322    |
| Lemmes                         | Mots outils, $\leq 3$ | 0.7317    |
| Lemmes                         | $\leq 2$              | 0.7293    |
| Lemmes                         | $\leq 4$              | 0.7292    |
| Tokens                         | $\leq 4$              | 0.7275    |
| Tokens                         | $\leq 2$              | 0.7274    |
| Lemmes                         | $\leq 1$              | 0.7269    |
| Lemmes                         | $\leq 3$              | 0.7253    |
| Tokens                         | $\leq 3$              | 0.7219    |
| Lemmes                         | Mots outils, $\leq 4$ | 0.7202    |
| Étiquettes des entités nommées | Mots outils           | 0.7153    |
| Étiquettes des entités nommées | Mots outils, $\leq 1$ | 0.7153    |
| Étiquettes des entités nommées | Mots outils, $\leq 2$ | 0.7153    |
| Étiquettes des entités nommées | $\leq 1$              | 0.7153    |
| Étiquettes des entités nommées | $\leq 2$              | 0.7153    |
| Étiquettes des entités nommées | Pas de filtre         | 0.7153    |
| Lemmes                         | Pas de filtre         | 0.7148    |
| Tokens                         | $\leq 1$              | 0.7135    |
| Tokens                         | Pas de filtre         | 0.7098    |
| Étiquettes des entités nommées | Mots outils, $\leq 3$ | 0.7081    |
| Étiquettes des entités nommées | $\leq 3$              | 0.7081    |

\newpage
*année 2017*

| Sac de mots                    | Filtres               | Score MAP |
|--------------------------------|-----------------------|-----------|
| Tokens                         | Mots outils, $\leq 1$ | 0.4705    |
| Lemmes                         | Mots outils, $\leq 1$ | 0.4678    |
| Tokens                         | Mots outils, $\leq 2$ | 0.4653    |
| Tokens                         | Mots outils, $\leq 3$ | 0.4650    |
| Lemmes                         | Mots outils, $\leq 2$ | 0.4624    |
| Tokens                         | $\leq 2$              | 0.4623    |
| Tokens                         | Mots outils           | 0.4598    |
| Lemmes                         | Mots outils           | 0.4581    |
| Lemmes                         | Pas de filtre         | 0.4580    |
| Tokens                         | Mots outils, $\leq 4$ | 0.4521    |
| Lemmes                         | $\leq 1$              | 0.4473    |
| Lemmes                         | Mots outils, $\leq 3$ | 0.4458    |
| Tokens                         | $\leq 1$              | 0.4455    |
| Lemmes                         | $\leq 2$              | 0.4425    |
| Lemmes                         | Mots outils, $\leq 4$ | 0.4419    |
| Tokens                         | Pas de filtre         | 0.4418    |
| Tokens                         | $\leq 3$              | 0.4392    |
| Lemmes                         | $\leq 3$              | 0.4389    |
| Lemmes                         | $\leq 4$              | 0.4249    |
| Tokens                         | $\leq 4$              | 0.4153    |
| Textes des entités nommées     | Mots outils, $\leq 3$ | 0.4139    |
| Textes des entités nommées     | Mots outils, $\leq 4$ | 0.4139    |
| Textes des entités nommées     | $\leq 3$              | 0.4139    |
| Textes des entités nommées     | $\leq 4$              | 0.4139    |
| Étiquettes des entités nommées | Mots outils           | 0.4123    |
| Étiquettes des entités nommées | Mots outils, $\leq 1$ | 0.4123    |
| Étiquettes des entités nommées | Mots outils, $\leq 2$ | 0.4123    |
| Étiquettes des entités nommées | $\leq 1$              | 0.4123    |
| Étiquettes des entités nommées | $\leq 2$              | 0.4123    |
| Étiquettes des entités nommées | Pas de filtre         | 0.4123    |
| Étiquettes des entités nommées | Mots outils, $\leq 3$ | 0.4104    |
| Étiquettes des entités nommées | $\leq 3$              | 0.4104    |
| Textes des entités nommées     | Mots outils           | 0.4083    |
| Textes des entités nommées     | Mots outils, $\leq 1$ | 0.4083    |
| Textes des entités nommées     | Mots outils, $\leq 2$ | 0.4083    |
| Textes des entités nommées     | $\leq 1$              | 0.4083    |
| Textes des entités nommées     | $\leq 2$              | 0.4083    |
| Textes des entités nommées     | Pas de filtre         | 0.4083    |
| Étiquettes des entités nommées | Mots outils, $\leq 4$ | 0.4063    |
| Étiquettes des entités nommées | $\leq 4$              | 0.4063    |

:END:


| Année | Score MAP baseline |
|-------+--------------------|
|  2016 |             0.7475 |
|  2017 |             0.4185 |



*** Méthodes pondérées

Le but des méthodes pondérées est d'utiliser plusieurs indicateurs au sein d'une même méthode.
Un exemple d'approche de pondération est d'utiliser les lemmes pour estimer la similarité de phrases,
et de donner une plus grande importance aux lemmes communs qui sont également des entités nommées.

**** Recherche des pondérations optimales
**** Pondération par entités nommées

#+BEGIN_SRC ipython
  ponderated_methods = (doctrees, morphologic_indicators, filters_partition)

  for doctree, indicator, fltrs in product(*ponderated_methods):
      wordex, sentex = all_indicators[indicator]

      customscorer = scorer(
          wordextractors[wordex],
          sentenceextractors[sentex],
          [filters[filterkey] for filterkey in fltrs],
          inversedocfreqs[wordex + '_' + sentex],
          out_of_corpus_value,
          lambda this, a, b : generic_weights_scorer(this, a, b, [(0.6, entity_weighter)])
      )
      scores = make_score_tree(
          doctrees[doctree],
          customscorer.get_score
          # lambda a, b: entityweight_scorer(
          #     wordextractors[wordex],
          #     [filters[filterkey] for filterkey in fltrs],
          #     a, b, inversedocfreqs[wordex + '_' + sentex],
          #     out_of_corpus_value
          # )
      )

      prediction_file = getpredfilename(doctree, indicator, fltrs, 'nerponderation')
      write_scores_to_file(scores, prediction_file, verbose=True)
#+END_SRC

#+BEGIN_SRC ipython :tangle no :exports results :results output drawer replace
  for doctree in doctrees:
      restable = [[get_indicator_descr(indi),
                   get_filters_descr(fltr),
                   get_map_score(getpredfilename(doctree, indi, fltr, 'nerponderation'))]
                  for indi, fltr in product(*ponderated_methods[1:])]

      restable.sort(key=lambda x: x[2], reverse=True)
      restable.insert(0, ['Sac de mots', 'Filtres', 'Score MAP'])
      print('\\newpage\n' + '*année ' + all_doctrees_descr[doctree] + '*' + '\n')
      print(orgmodetable(restable, header=True))
      print()
#+END_SRC

#+RESULTS:
:RESULTS:
\newpage
*année 2016*

| Sac de mots | Filtres               | Score MAP |
|-------------|-----------------------|-----------|
| Lemmes      | Mots outils, $\leq 2$ | 0.7663    |
| Tokens      | Mots outils, $\leq 2$ | 0.7497    |
| Lemmes      | Mots outils, $\leq 1$ | 0.7474    |
| Tokens      | Mots outils, $\leq 1$ | 0.7446    |
| Tokens      | Mots outils, $\leq 3$ | 0.7430    |
| Tokens      | Mots outils, $\leq 4$ | 0.7422    |
| Lemmes      | Mots outils           | 0.7405    |
| Tokens      | Mots outils           | 0.7319    |
| Lemmes      | Mots outils, $\leq 3$ | 0.7301    |
| Lemmes      | $\leq 4$              | 0.7292    |
| Lemmes      | $\leq 2$              | 0.7287    |
| Tokens      | $\leq 4$              | 0.7264    |
| Tokens      | $\leq 2$              | 0.7254    |
| Lemmes      | $\leq 1$              | 0.7239    |
| Lemmes      | $\leq 3$              | 0.7237    |
| Tokens      | $\leq 3$              | 0.7214    |
| Lemmes      | Mots outils, $\leq 4$ | 0.7202    |
| Lemmes      | Pas de filtre         | 0.7167    |
| Tokens      | $\leq 1$              | 0.7142    |
| Tokens      | Pas de filtre         | 0.7078    |

\newpage
*année 2017*

| Sac de mots | Filtres               | Score MAP |
|-------------|-----------------------|-----------|
| Tokens      | Mots outils, $\leq 1$ | 0.4725    |
| Lemmes      | Mots outils, $\leq 1$ | 0.4707    |
| Tokens      | Mots outils, $\leq 3$ | 0.4658    |
| Tokens      | Mots outils, $\leq 2$ | 0.4651    |
| Tokens      | Mots outils           | 0.4622    |
| Tokens      | $\leq 2$              | 0.4621    |
| Lemmes      | Mots outils, $\leq 2$ | 0.4618    |
| Lemmes      | Mots outils           | 0.4599    |
| Tokens      | Mots outils, $\leq 4$ | 0.4521    |
| Lemmes      | Pas de filtre         | 0.4509    |
| Lemmes      | $\leq 1$              | 0.4477    |
| Tokens      | $\leq 1$              | 0.4475    |
| Lemmes      | Mots outils, $\leq 3$ | 0.4432    |
| Lemmes      | $\leq 2$              | 0.4424    |
| Lemmes      | Mots outils, $\leq 4$ | 0.4413    |
| Tokens      | $\leq 3$              | 0.4412    |
| Tokens      | Pas de filtre         | 0.4412    |
| Lemmes      | $\leq 3$              | 0.4387    |
| Lemmes      | $\leq 4$              | 0.4252    |
| Tokens      | $\leq 4$              | 0.4124    |

:END:

* TODO student t test
et insister sur l'analyse manuelle, en particulier de ce qui n'a pas marché

* Debug                          :noexport:
#+BEGIN_SRC ipython :results output replace drawer :eval noexport :session semexec :tangle no

  if debug_mode:
      for filterspartition in filters_partition:
          wordex, sentex = 'lemma', 'document'

          customscorer = scorer(
              wordextractors[wordex],
              sentenceextractors[sentex],
              [filters[filterkey] for filterkey in filterspartition],
              inversedocfreqs[wordex + '_' + sentex],
              out_of_corpus_value,
              lambda self, a, b : generic_weights_scorer(self, a, b, [(0.6, entity_weighter)])
  #            tf_idf_scorer
          )

          scores = make_score_tree(
              doctrees[doctree],
              lambda a, b: customscorer.get_score(
                  a, b))

          printsubset = {'Q318'}
          print({k: scores[k] for k in scores.keys() & printsubset})
          print()
          # prediction_file = getpredfilename('spacy_en_2016_questions', 'named_entities_label', filterspartition)
          # print('writing scores to', prediction_file)
          # write_scores_to_file(scores, prediction_file)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


* perspectives

 - Ajouter des dimensions (catégorie grammaticale, etc) et ne conserver que les $n$ meilleurs et les $n$ pires, en partant du principe qu'il est plus intéressant d'analyser les combinaisons de paramètres ne fonctionnant pas et celles fonctionnant.


dictionnaire synonymes
+ de filtres
combinaison entités et (lemmes ou texte)
catégories grammaticales

le score 
le nombre de 
